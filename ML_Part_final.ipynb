{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "nGtVStKtv6LT",
    "outputId": "500f07cd-2f74-4ae5-8fce-1df0ba4565bc"
   },
   "outputs": [],
   "source": [
    "SERVER = False\n",
    "\n",
    "# Files and Directories\n",
    "ROOT_DIR = \"\"\n",
    "TRAIN_PATH = 'wav_train_subset'\n",
    "STFT_FOLDER = 'stft'\n",
    "CHECKPOINTS_FOLDER = \"checkpoints\"\n",
    "PAIRS_FILE = 'pairs.csv'\n",
    "VGG_VOX_WEIGHT_FILE = \"vggvox_ident_net.mat\"\n",
    "\n",
    "# Data_Part\n",
    "TOTAL_USERS = 100\n",
    "CLIPS_PER_USER = 10\n",
    "MIN_CLIP_DURATION = 3.\n",
    "\n",
    "# ML_Part\n",
    "TRAINING_USERS = 80\n",
    "SIMILAR_PAIRS = 20\n",
    "DISSIMILAR_PAIRS = SIMILAR_PAIRS\n",
    "\n",
    "assert SIMILAR_PAIRS <= CLIPS_PER_USER * (CLIPS_PER_USER - 1)\n",
    "# print(\"len of pairs.csv should be\", (SIMILAR_PAIRS + DISSIMILAR_PAIRS) * TOTAL_USERS)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from IPython.core.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import librosa\n",
    "import librosa.display\n",
    "import wave\n",
    "import contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from network import *\n",
    "\n",
    "\n",
    "assert os.path.exists(STFT_FOLDER)\n",
    "assert os.path.exists(CHECKPOINTS_FOLDER)\n",
    "\n",
    "def get_rel_path(path, server=SERVER, root_dir=ROOT_DIR):\n",
    "    if server:\n",
    "        return os.path.join(root_dir, path)\n",
    "    else:\n",
    "        return path\n",
    "\n",
    "\n",
    "def wavPlayer(filepath):\n",
    "    src = \"\"\"\n",
    "    <head>\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
    "    <title>Simple Test</title>\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "    <audio controls=\"controls\" style=\"width:600px\" >\n",
    "      <source src=\"%s\" type=\"audio/wav\" />\n",
    "      Your browser does not support the audio element.\n",
    "    </audio>\n",
    "    </body>\n",
    "    \"\"\"%(filepath)\n",
    "    display(HTML(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i6aZ0NykBT70",
    "outputId": "134978d1-3f04-456d-d08e-d382ba6e0629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "1swM4U97IAeK",
    "outputId": "d745b67b-6e68-43be-efa1-9feeea9c7551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7', 'fc8'])\n",
      "conv1 [(96, 1, 7, 7), (96,)]\n",
      "conv2 [(256, 96, 5, 5), (256,)]\n",
      "conv3 [(384, 256, 3, 3), (384,)]\n",
      "conv4 [(256, 384, 3, 3), (256,)]\n",
      "conv5 [(256, 256, 3, 3), (256,)]\n",
      "fc6 [(4096, 256, 9, 1), (4096,)]\n",
      "fc7 [(1024, 4096, 1, 1), (1024,)]\n",
      "fc8 [(1300, 1024), (1300,)]\n"
     ]
    }
   ],
   "source": [
    "weights = {}\n",
    "\n",
    "# loading pretrained vog_vgg learned weights\n",
    "vox_weights = loadmat(get_rel_path(VGG_VOX_WEIGHT_FILE), \n",
    "                      struct_as_record=False, squeeze_me=True)\n",
    "\n",
    "for l in vox_weights['net'].layers[:-1]:\n",
    "    if len(l.weights) > 0:\n",
    "        weights[l.name] = l.weights\n",
    "#         print(l.name, [i.shape for i in l.weights])\n",
    "        \n",
    "for i in weights:\n",
    "    weights[i][0] = weights[i][0].T \n",
    "\n",
    "weights['conv1'][0] = np.expand_dims(weights['conv1'][0], axis=1)\n",
    "weights['fc6'][0] = np.expand_dims(weights['fc6'][0], axis=3)\n",
    "weights['fc7'][0] = np.expand_dims(weights['fc7'][0], axis=-1)\n",
    "weights['fc7'][0] = np.expand_dims(weights['fc7'][0], axis=-1)\n",
    "\n",
    "print(weights.keys())   \n",
    "for key in weights:\n",
    "    print(key, [i.shape for i in weights[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0jMmErb3Nv-"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "conv_kernel1, n_f1, s1, p1 = 7, 96, 2, 1\n",
    "pool_kernel1, pool_s1 = 3, 2\n",
    "\n",
    "conv_kernel2, n_f2, s2, p2 = 5, 256, 2, 1\n",
    "pool_kernel2, pool_s2 = 3, 2\n",
    "\n",
    "conv_kernel3, n_f3, s3, p3 = 3, 384, 1, 1\n",
    "\n",
    "conv_kernel4, n_f4, s4, p4 = 3, 256, 1, 1\n",
    "\n",
    "conv_kernel5, n_f5, s5, p5 = 3, 256, 1, 1\n",
    "pool_kernel5_x, pool_kernel5_y, pool_s5_x, pool_s5_y = 5, 3, 3, 2\n",
    "\n",
    "conv_kernel6_x, conv_kernel6_y, n_f6, s6 = 9, 1, 4096, 1\n",
    "\n",
    "conv_kernel7, n_f7, s7 = 1, 1024, 1\n",
    "\n",
    "conv_kernel8, n_f8, s8 = 1, 1024, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S88jByoxIA0w"
   },
   "outputs": [],
   "source": [
    "class VggVox(nn.Module):\n",
    "    '''\n",
    "    Class for CNN architecture (VGGvox)\n",
    "    '''\n",
    "    def __init__(self, use_weights=True):\n",
    "        super(VggVox, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1, \n",
    "                                             out_channels=n_f1, \n",
    "                                             kernel_size=conv_kernel1,\n",
    "                                             stride=s1,\n",
    "                                             padding=p1),\n",
    "                                    nn.BatchNorm2d(num_features=n_f1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=pool_kernel1,\n",
    "                                                 stride=pool_s1))           \n",
    "            \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=n_f1,\n",
    "                                         out_channels=n_f2,\n",
    "                                         kernel_size=conv_kernel2,\n",
    "                                         stride=s2,\n",
    "                                         padding=p2),\n",
    "                                nn.BatchNorm2d(num_features=n_f2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=pool_kernel2,\n",
    "                                             stride=pool_s2))\n",
    "\n",
    "\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=n_f2,\n",
    "                                             out_channels=n_f3,\n",
    "                                             kernel_size=conv_kernel3,\n",
    "                                             stride=s3,\n",
    "                                             padding=p3),\n",
    "                                    nn.BatchNorm2d(num_features=n_f3),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=n_f3,\n",
    "                                             out_channels=n_f4,\n",
    "                                             kernel_size=conv_kernel4,\n",
    "                                             stride=s4,\n",
    "                                             padding=p4),\n",
    "                                   nn.BatchNorm2d(num_features=n_f4),\n",
    "                                   nn.ReLU())\n",
    "\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels=n_f4,\n",
    "                                             out_channels=n_f5,\n",
    "                                             kernel_size=conv_kernel5,\n",
    "                                             stride=s5,\n",
    "                                             padding=p5),\n",
    "                                   nn.BatchNorm2d(num_features=n_f5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(kernel_size=(pool_kernel5_x, pool_kernel5_y),\n",
    "                                                stride=(pool_s5_x, pool_s5_y)))\n",
    "\n",
    "        self.fc6 = nn.Sequential(nn.Conv2d(in_channels=n_f5,\n",
    "                                             out_channels=n_f6,\n",
    "                                             kernel_size=(conv_kernel6_x, conv_kernel6_y),\n",
    "                                             stride=s6),\n",
    "                                 nn.BatchNorm2d(num_features=n_f6), \n",
    "                                 nn.ReLU())\n",
    "\n",
    "        self.global_pool = nn.AvgPool2d\n",
    "\n",
    "        self.fc7 = nn.Sequential(nn.Conv2d(in_channels=n_f6,\n",
    "                                           out_channels=n_f7,\n",
    "                                           kernel_size=conv_kernel7,\n",
    "                                           stride=s7),\n",
    "                                 nn.ReLU())\n",
    "\n",
    "        self.fc8 = nn.Sequential(nn.Conv2d(in_channels=n_f7,\n",
    "                                           out_channels=n_f8,\n",
    "                                           kernel_size=conv_kernel8,\n",
    "                                           stride=s8))\n",
    "   \n",
    "        if use_weights:\n",
    "            self.conv1[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv1'][0]))\n",
    "            self.conv1[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv1'][1]))\n",
    "            \n",
    "            self.conv2[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv2'][0]))\n",
    "            self.conv2[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv2'][1]))\n",
    "            \n",
    "            self.conv3[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv3'][0]))\n",
    "            self.conv3[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv3'][1]))\n",
    "            \n",
    "            self.conv4[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv4'][0]))\n",
    "            self.conv4[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv4'][1]))\n",
    "            \n",
    "            self.conv5[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv5'][0]))\n",
    "            self.conv5[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv5'][1]))            \n",
    "            \n",
    "            self.fc6[0].weight = torch.nn.Parameter(torch.from_numpy(weights['fc6'][0]))\n",
    "            self.fc6[1].weight = torch.nn.Parameter(torch.from_numpy(weights['fc6'][1]))\n",
    "            \n",
    "            self.fc7[0].weight = torch.nn.Parameter(torch.from_numpy(weights['fc7'][0]))\n",
    "   \n",
    "    def forward_single(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.global_pool(kernel_size=x.size()[2:])(x)\n",
    "        x = self.fc7(x)\n",
    "        out = self.fc8(x)      \n",
    "        out = out.view(-1, out.shape[1])\n",
    "        return out\n",
    "  \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_single(input1)\n",
    "        output2 = self.forward_single(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GNo5xzFILc6"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91Xm-2JiILyF"
   },
   "outputs": [],
   "source": [
    "class VoxCelebDataset(Dataset):\n",
    "    def __init__(self, pairs_fname=PAIRS_FILE, n_users=TRAINING_USERS, clips_per_user=CLIPS_PER_USER):\n",
    "        pairs_file = pd.read_csv(get_rel_path(pairs_fname))\n",
    "        self.all_user_ids = sorted(pairs_file.user1.unique())\n",
    "        self.training_users = self.all_user_ids[: n_users]\n",
    "        \n",
    "        user1_subset = pairs_file[pairs_file.user1.isin(self.training_users)]\n",
    "        user2_subset = user1_subset[user1_subset.user2.isin(self.training_users)]\n",
    "        \n",
    "        def balance_data(df):\n",
    "            pairs_df = []\n",
    "            for user in df.user1.unique():\n",
    "                user_df = df[df.user1 == user]\n",
    "                similar = user_df[user_df['label'] == 0].sample(n=SIMILAR_PAIRS)\n",
    "                dissimilar = user_df[user_df['label'] == 1].sample(n=DISSIMILAR_PAIRS)\n",
    "                pairs_df.append(pd.concat([similar, dissimilar]))\n",
    "\n",
    "            pairs_df = pd.concat(pairs_df)\n",
    "            return pairs_df\n",
    "                \n",
    "        pairs_df = balance_data(user2_subset)\n",
    "        \n",
    "        assert len(pairs_df[pairs_df.user1.isin(self.training_users)]) == len(pairs_df)\n",
    "        assert len(pairs_df[pairs_df.user2.isin(self.training_users)]) == len(pairs_df)\n",
    "        \n",
    "        self.spec = pairs_df[['path1', 'path2', 'label']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spec)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spec1_path = get_rel_path(self.spec[idx][0])\n",
    "        spec2_path = get_rel_path(self.spec[idx][1])\n",
    "        label = int(self.spec[idx][2])\n",
    "        \n",
    "        spec1 = np.load(spec1_path)\n",
    "        spec2 = np.load(spec2_path)\n",
    "        \n",
    "        spec1 = np.expand_dims(spec1, axis=0)\n",
    "        spec2 = np.expand_dims(spec2, axis=0)\n",
    "        \n",
    "        assert spec1.ndim == 3, spec2.ndim == 3\n",
    "        \n",
    "        sample = {'spec1': spec1, 'spec2': spec2, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z100FbkIYslN"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, loss):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    fname = \"checkpoint_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \"_\" + str(loss.item()) + \".pth.tar\"\n",
    "    torch.save(state, get_rel_path(os.path.join(CHECKPOINTS_FOLDER, fname)))  # save checkpoint\n",
    "    print(\"$$$ Saved a new checkpoint\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfoWVXiKIWEm"
   },
   "outputs": [],
   "source": [
    "def load_saved_model(fname, test=True):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    new_model_dict = VggVox()\n",
    "    checkpoint_path = get_rel_path(os.path.join(CHECKPOINTS_FOLDER, fname))\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    new_model_dict.load_state_dict(checkpoint['state_dict'])\n",
    "    if test:\n",
    "        model = new_model_dict.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    new_optimizer = optim.Adam(params=model.parameters())\n",
    "    new_optimizer.load_state_dict(checkpoint['optim_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    return model, new_model_dict, new_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bh80WU4lJAxE"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "lIKadKjFDTgx",
    "outputId": "6f187bdb-e63d-4e9b-d20f-b5d67cea8a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training unique users 80\n",
      "training samples 3200\n",
      "batches 100\n"
     ]
    }
   ],
   "source": [
    "voxceleb_dataset = VoxCelebDataset(PAIRS_FILE)\n",
    "train_dataloader = DataLoader(voxceleb_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              num_workers=4)\n",
    "n_batches = int(len(voxceleb_dataset) / BATCH_SIZE)\n",
    "\n",
    "print(\"training unique users\", len(voxceleb_dataset.training_users))\n",
    "print(\"training samples\", len(voxceleb_dataset))\n",
    "print(\"batches\", int(len(voxceleb_dataset) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coB4oXXDIOqp"
   },
   "outputs": [],
   "source": [
    "model = VggVox(use_weights=True)\n",
    "model = model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnjSEjKOwjGV"
   },
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss()\n",
    "criterion = criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDMrogyLwjGY"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUQYaOUjwjGb"
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "best_loss = torch.autograd.Variable(torch.tensor(np.inf)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3519
    },
    "colab_type": "code",
    "id": "FbWVd_VNIVYr",
    "outputId": "9b55d499-8dcf-4f54-80de-f208b94acdd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  Batch 5/100 \n",
      "Current Batch Loss 8.436244010925293\n",
      "\n",
      "Epoch 1/30  Batch 10/100 \n",
      "Current Batch Loss 1.8622654676437378\n",
      "\n",
      "Epoch 1/30  Batch 15/100 \n",
      "Current Batch Loss 1.4234672784805298\n",
      "\n",
      "Epoch 1/30  Batch 20/100 \n",
      "Current Batch Loss 1.0617985725402832\n",
      "\n",
      "Epoch 1/30  Batch 25/100 \n",
      "Current Batch Loss 1.100848913192749\n",
      "\n",
      "Epoch 1/30  Batch 30/100 \n",
      "Current Batch Loss 0.8522859215736389\n",
      "\n",
      "Epoch 1/30  Batch 35/100 \n",
      "Current Batch Loss 1.2844592332839966\n",
      "\n",
      "Epoch 1/30  Batch 40/100 \n",
      "Current Batch Loss 1.4136996269226074\n",
      "\n",
      "Epoch 1/30  Batch 45/100 \n",
      "Current Batch Loss 0.8286517858505249\n",
      "\n",
      "Epoch 1/30  Batch 50/100 \n",
      "Current Batch Loss 1.0743564367294312\n",
      "\n",
      "Epoch 1/30  Batch 55/100 \n",
      "Current Batch Loss 0.9677821397781372\n",
      "\n",
      "Epoch 1/30  Batch 60/100 \n",
      "Current Batch Loss 1.0197561979293823\n",
      "\n",
      "Epoch 1/30  Batch 65/100 \n",
      "Current Batch Loss 0.9938297271728516\n",
      "\n",
      "Epoch 1/30  Batch 70/100 \n",
      "Current Batch Loss 0.9094753265380859\n",
      "\n",
      "Epoch 1/30  Batch 75/100 \n",
      "Current Batch Loss 1.0850576162338257\n",
      "\n",
      "Epoch 1/30  Batch 80/100 \n",
      "Current Batch Loss 1.0233205556869507\n",
      "\n",
      "Epoch 1/30  Batch 85/100 \n",
      "Current Batch Loss 1.524418592453003\n",
      "\n",
      "Epoch 1/30  Batch 90/100 \n",
      "Current Batch Loss 1.0170056819915771\n",
      "\n",
      "Epoch 1/30  Batch 95/100 \n",
      "Current Batch Loss 1.0588668584823608\n",
      "\n",
      "Epoch 1/30  Batch 100/100 \n",
      "Current Batch Loss 1.362423062324524\n",
      "\n",
      "==> Epoch 1/30 Epoch Loss 0.14750675857067108\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 2/30  Batch 5/100 \n",
      "Current Batch Loss 1.0458623170852661\n",
      "\n",
      "Epoch 2/30  Batch 10/100 \n",
      "Current Batch Loss 0.9987772107124329\n",
      "\n",
      "Epoch 2/30  Batch 15/100 \n",
      "Current Batch Loss 0.7552749514579773\n",
      "\n",
      "Epoch 2/30  Batch 20/100 \n",
      "Current Batch Loss 0.7340353727340698\n",
      "\n",
      "Epoch 2/30  Batch 25/100 \n",
      "Current Batch Loss 0.9852666258811951\n",
      "\n",
      "Epoch 2/30  Batch 30/100 \n",
      "Current Batch Loss 0.8037605881690979\n",
      "\n",
      "Epoch 2/30  Batch 35/100 \n",
      "Current Batch Loss 1.0594569444656372\n",
      "\n",
      "Epoch 2/30  Batch 40/100 \n",
      "Current Batch Loss 0.9886955618858337\n",
      "\n",
      "Epoch 2/30  Batch 45/100 \n",
      "Current Batch Loss 0.6052592992782593\n",
      "\n",
      "Epoch 2/30  Batch 50/100 \n",
      "Current Batch Loss 1.1657867431640625\n",
      "\n",
      "Epoch 2/30  Batch 55/100 \n",
      "Current Batch Loss 1.0644069910049438\n",
      "\n",
      "Epoch 2/30  Batch 60/100 \n",
      "Current Batch Loss 0.5586414337158203\n",
      "\n",
      "Epoch 2/30  Batch 65/100 \n",
      "Current Batch Loss 0.691573441028595\n",
      "\n",
      "Epoch 2/30  Batch 70/100 \n",
      "Current Batch Loss 0.8378567695617676\n",
      "\n",
      "Epoch 2/30  Batch 75/100 \n",
      "Current Batch Loss 0.7236993312835693\n",
      "\n",
      "Epoch 2/30  Batch 80/100 \n",
      "Current Batch Loss 0.8432947397232056\n",
      "\n",
      "Epoch 2/30  Batch 85/100 \n",
      "Current Batch Loss 0.7866047620773315\n",
      "\n",
      "Epoch 2/30  Batch 90/100 \n",
      "Current Batch Loss 0.8187037706375122\n",
      "\n",
      "Epoch 2/30  Batch 95/100 \n",
      "Current Batch Loss 0.524632453918457\n",
      "\n",
      "Epoch 2/30  Batch 100/100 \n",
      "Current Batch Loss 0.7634215354919434\n",
      "\n",
      "==> Epoch 2/30 Epoch Loss 0.025060191750526428\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 3/30  Batch 5/100 \n",
      "Current Batch Loss 0.8291268348693848\n",
      "\n",
      "Epoch 3/30  Batch 10/100 \n",
      "Current Batch Loss 0.5552381873130798\n",
      "\n",
      "Epoch 3/30  Batch 15/100 \n",
      "Current Batch Loss 0.5469911694526672\n",
      "\n",
      "Epoch 3/30  Batch 20/100 \n",
      "Current Batch Loss 0.5938701629638672\n",
      "\n",
      "Epoch 3/30  Batch 25/100 \n",
      "Current Batch Loss 0.3978380858898163\n",
      "\n",
      "Epoch 3/30  Batch 30/100 \n",
      "Current Batch Loss 0.5727263689041138\n",
      "\n",
      "Epoch 3/30  Batch 35/100 \n",
      "Current Batch Loss 0.6580319404602051\n",
      "\n",
      "Epoch 3/30  Batch 40/100 \n",
      "Current Batch Loss 0.5735681056976318\n",
      "\n",
      "Epoch 3/30  Batch 45/100 \n",
      "Current Batch Loss 0.5423517227172852\n",
      "\n",
      "Epoch 3/30  Batch 50/100 \n",
      "Current Batch Loss 0.6706011295318604\n",
      "\n",
      "Epoch 3/30  Batch 55/100 \n",
      "Current Batch Loss 0.7641209363937378\n",
      "\n",
      "Epoch 3/30  Batch 60/100 \n",
      "Current Batch Loss 0.8065405488014221\n",
      "\n",
      "Epoch 3/30  Batch 65/100 \n",
      "Current Batch Loss 0.7563888430595398\n",
      "\n",
      "Epoch 3/30  Batch 70/100 \n",
      "Current Batch Loss 0.7870856523513794\n",
      "\n",
      "Epoch 3/30  Batch 75/100 \n",
      "Current Batch Loss 0.6631565093994141\n",
      "\n",
      "Epoch 3/30  Batch 80/100 \n",
      "Current Batch Loss 0.7924984097480774\n",
      "\n",
      "Epoch 3/30  Batch 85/100 \n",
      "Current Batch Loss 0.8017359375953674\n",
      "\n",
      "Epoch 3/30  Batch 90/100 \n",
      "Current Batch Loss 0.6618489027023315\n",
      "\n",
      "Epoch 3/30  Batch 95/100 \n",
      "Current Batch Loss 0.6166315674781799\n",
      "\n",
      "Epoch 3/30  Batch 100/100 \n",
      "Current Batch Loss 0.5591933131217957\n",
      "\n",
      "==> Epoch 3/30 Epoch Loss 0.020651986822485924\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 4/30  Batch 5/100 \n",
      "Current Batch Loss 0.5603134036064148\n",
      "\n",
      "Epoch 4/30  Batch 10/100 \n",
      "Current Batch Loss 0.9892481565475464\n",
      "\n",
      "Epoch 4/30  Batch 15/100 \n",
      "Current Batch Loss 0.5573372840881348\n",
      "\n",
      "Epoch 4/30  Batch 20/100 \n",
      "Current Batch Loss 0.5532354116439819\n",
      "\n",
      "Epoch 4/30  Batch 25/100 \n",
      "Current Batch Loss 0.6473094820976257\n",
      "\n",
      "Epoch 4/30  Batch 30/100 \n",
      "Current Batch Loss 0.47807827591896057\n",
      "\n",
      "Epoch 4/30  Batch 35/100 \n",
      "Current Batch Loss 0.5714914202690125\n",
      "\n",
      "Epoch 4/30  Batch 40/100 \n",
      "Current Batch Loss 0.6103043556213379\n",
      "\n",
      "Epoch 4/30  Batch 45/100 \n",
      "Current Batch Loss 0.7782586812973022\n",
      "\n",
      "Epoch 4/30  Batch 50/100 \n",
      "Current Batch Loss 0.5279734134674072\n",
      "\n",
      "Epoch 4/30  Batch 55/100 \n",
      "Current Batch Loss 0.27720946073532104\n",
      "\n",
      "Epoch 4/30  Batch 60/100 \n",
      "Current Batch Loss 0.637955904006958\n",
      "\n",
      "Epoch 4/30  Batch 65/100 \n",
      "Current Batch Loss 0.7504391074180603\n",
      "\n",
      "Epoch 4/30  Batch 70/100 \n",
      "Current Batch Loss 0.37985265254974365\n",
      "\n",
      "Epoch 4/30  Batch 75/100 \n",
      "Current Batch Loss 0.832179069519043\n",
      "\n",
      "Epoch 4/30  Batch 80/100 \n",
      "Current Batch Loss 0.5320493578910828\n",
      "\n",
      "Epoch 4/30  Batch 85/100 \n",
      "Current Batch Loss 0.47845321893692017\n",
      "\n",
      "Epoch 4/30  Batch 90/100 \n",
      "Current Batch Loss 0.39810889959335327\n",
      "\n",
      "Epoch 4/30  Batch 95/100 \n",
      "Current Batch Loss 0.6368630528450012\n",
      "\n",
      "Epoch 4/30  Batch 100/100 \n",
      "Current Batch Loss 0.4326591193675995\n",
      "\n",
      "==> Epoch 4/30 Epoch Loss 0.018174389377236366\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 5/30  Batch 5/100 \n",
      "Current Batch Loss 0.3228091299533844\n",
      "\n",
      "Epoch 5/30  Batch 10/100 \n",
      "Current Batch Loss 0.36587679386138916\n",
      "\n",
      "Epoch 5/30  Batch 15/100 \n",
      "Current Batch Loss 0.46980446577072144\n",
      "\n",
      "Epoch 5/30  Batch 20/100 \n",
      "Current Batch Loss 0.6727197170257568\n",
      "\n",
      "Epoch 5/30  Batch 25/100 \n",
      "Current Batch Loss 0.41411077976226807\n",
      "\n",
      "Epoch 5/30  Batch 30/100 \n",
      "Current Batch Loss 0.45705974102020264\n",
      "\n",
      "Epoch 5/30  Batch 35/100 \n",
      "Current Batch Loss 0.5056363344192505\n",
      "\n",
      "Epoch 5/30  Batch 40/100 \n",
      "Current Batch Loss 0.5479257702827454\n",
      "\n",
      "Epoch 5/30  Batch 45/100 \n",
      "Current Batch Loss 0.3984839618206024\n",
      "\n",
      "Epoch 5/30  Batch 50/100 \n",
      "Current Batch Loss 0.8498300313949585\n",
      "\n",
      "Epoch 5/30  Batch 55/100 \n",
      "Current Batch Loss 0.3746916651725769\n",
      "\n",
      "Epoch 5/30  Batch 60/100 \n",
      "Current Batch Loss 0.5248689651489258\n",
      "\n",
      "Epoch 5/30  Batch 65/100 \n",
      "Current Batch Loss 0.7505630254745483\n",
      "\n",
      "Epoch 5/30  Batch 70/100 \n",
      "Current Batch Loss 0.27957862615585327\n",
      "\n",
      "Epoch 5/30  Batch 75/100 \n",
      "Current Batch Loss 0.48202747106552124\n",
      "\n",
      "Epoch 5/30  Batch 80/100 \n",
      "Current Batch Loss 0.4369044899940491\n",
      "\n",
      "Epoch 5/30  Batch 85/100 \n",
      "Current Batch Loss 0.5685542821884155\n",
      "\n",
      "Epoch 5/30  Batch 90/100 \n",
      "Current Batch Loss 0.5796678066253662\n",
      "\n",
      "Epoch 5/30  Batch 95/100 \n",
      "Current Batch Loss 0.5623872876167297\n",
      "\n",
      "Epoch 5/30  Batch 100/100 \n",
      "Current Batch Loss 0.48882973194122314\n",
      "\n",
      "==> Epoch 5/30 Epoch Loss 0.015742819756269455\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 6/30  Batch 5/100 \n",
      "Current Batch Loss 0.3725501298904419\n",
      "\n",
      "Epoch 6/30  Batch 10/100 \n",
      "Current Batch Loss 0.32921844720840454\n",
      "\n",
      "Epoch 6/30  Batch 15/100 \n",
      "Current Batch Loss 0.47088855504989624\n",
      "\n",
      "Epoch 6/30  Batch 20/100 \n",
      "Current Batch Loss 0.7373554110527039\n",
      "\n",
      "Epoch 6/30  Batch 25/100 \n",
      "Current Batch Loss 0.4281127452850342\n",
      "\n",
      "Epoch 6/30  Batch 30/100 \n",
      "Current Batch Loss 0.7488856315612793\n",
      "\n",
      "Epoch 6/30  Batch 35/100 \n",
      "Current Batch Loss 0.39894768595695496\n",
      "\n",
      "Epoch 6/30  Batch 40/100 \n",
      "Current Batch Loss 0.29865097999572754\n",
      "\n",
      "Epoch 6/30  Batch 45/100 \n",
      "Current Batch Loss 0.3017084300518036\n",
      "\n",
      "Epoch 6/30  Batch 50/100 \n",
      "Current Batch Loss 0.27271270751953125\n",
      "\n",
      "Epoch 6/30  Batch 55/100 \n",
      "Current Batch Loss 0.45360562205314636\n",
      "\n",
      "Epoch 6/30  Batch 60/100 \n",
      "Current Batch Loss 0.655820906162262\n",
      "\n",
      "Epoch 6/30  Batch 65/100 \n",
      "Current Batch Loss 0.3674684762954712\n",
      "\n",
      "Epoch 6/30  Batch 70/100 \n",
      "Current Batch Loss 0.35992079973220825\n",
      "\n",
      "Epoch 6/30  Batch 75/100 \n",
      "Current Batch Loss 0.2712990343570709\n",
      "\n",
      "Epoch 6/30  Batch 80/100 \n",
      "Current Batch Loss 0.35753747820854187\n",
      "\n",
      "Epoch 6/30  Batch 85/100 \n",
      "Current Batch Loss 0.4349300265312195\n",
      "\n",
      "Epoch 6/30  Batch 90/100 \n",
      "Current Batch Loss 0.4420382082462311\n",
      "\n",
      "Epoch 6/30  Batch 95/100 \n",
      "Current Batch Loss 0.4215877950191498\n",
      "\n",
      "Epoch 6/30  Batch 100/100 \n",
      "Current Batch Loss 0.44962772727012634\n",
      "\n",
      "==> Epoch 6/30 Epoch Loss 0.014348131604492664\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 7/30  Batch 5/100 \n",
      "Current Batch Loss 0.4522892236709595\n",
      "\n",
      "Epoch 7/30  Batch 10/100 \n",
      "Current Batch Loss 0.31715884804725647\n",
      "\n",
      "Epoch 7/30  Batch 15/100 \n",
      "Current Batch Loss 0.6581928133964539\n",
      "\n",
      "Epoch 7/30  Batch 20/100 \n",
      "Current Batch Loss 0.3242705166339874\n",
      "\n",
      "Epoch 7/30  Batch 25/100 \n",
      "Current Batch Loss 0.40909212827682495\n",
      "\n",
      "Epoch 7/30  Batch 30/100 \n",
      "Current Batch Loss 0.4160574972629547\n",
      "\n",
      "Epoch 7/30  Batch 35/100 \n",
      "Current Batch Loss 0.3608614206314087\n",
      "\n",
      "Epoch 7/30  Batch 40/100 \n",
      "Current Batch Loss 0.5126456022262573\n",
      "\n",
      "Epoch 7/30  Batch 45/100 \n",
      "Current Batch Loss 0.6537479758262634\n",
      "\n",
      "Epoch 7/30  Batch 50/100 \n",
      "Current Batch Loss 0.3127822279930115\n",
      "\n",
      "Epoch 7/30  Batch 55/100 \n",
      "Current Batch Loss 0.42660391330718994\n",
      "\n",
      "Epoch 7/30  Batch 60/100 \n",
      "Current Batch Loss 0.36197125911712646\n",
      "\n",
      "Epoch 7/30  Batch 65/100 \n",
      "Current Batch Loss 0.35200875997543335\n",
      "\n",
      "Epoch 7/30  Batch 70/100 \n",
      "Current Batch Loss 0.5249243974685669\n",
      "\n",
      "Epoch 7/30  Batch 75/100 \n",
      "Current Batch Loss 0.4703768491744995\n",
      "\n",
      "Epoch 7/30  Batch 80/100 \n",
      "Current Batch Loss 0.4618830978870392\n",
      "\n",
      "Epoch 7/30  Batch 85/100 \n",
      "Current Batch Loss 0.2758629620075226\n",
      "\n",
      "Epoch 7/30  Batch 90/100 \n",
      "Current Batch Loss 0.5431451797485352\n",
      "\n",
      "Epoch 7/30  Batch 95/100 \n",
      "Current Batch Loss 0.49405422806739807\n",
      "\n",
      "Epoch 7/30  Batch 100/100 \n",
      "Current Batch Loss 0.44524824619293213\n",
      "\n",
      "==> Epoch 7/30 Epoch Loss 0.013070221990346909\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 8/30  Batch 5/100 \n",
      "Current Batch Loss 0.3020838797092438\n",
      "\n",
      "Epoch 8/30  Batch 10/100 \n",
      "Current Batch Loss 0.23994441330432892\n",
      "\n",
      "Epoch 8/30  Batch 15/100 \n",
      "Current Batch Loss 0.20094828307628632\n",
      "\n",
      "Epoch 8/30  Batch 20/100 \n",
      "Current Batch Loss 0.45345771312713623\n",
      "\n",
      "Epoch 8/30  Batch 25/100 \n",
      "Current Batch Loss 0.1421085149049759\n",
      "\n",
      "Epoch 8/30  Batch 30/100 \n",
      "Current Batch Loss 0.18307897448539734\n",
      "\n",
      "Epoch 8/30  Batch 35/100 \n",
      "Current Batch Loss 0.5440101027488708\n",
      "\n",
      "Epoch 8/30  Batch 40/100 \n",
      "Current Batch Loss 0.36900636553764343\n",
      "\n",
      "Epoch 8/30  Batch 45/100 \n",
      "Current Batch Loss 0.34134218096733093\n",
      "\n",
      "Epoch 8/30  Batch 50/100 \n",
      "Current Batch Loss 0.4048371911048889\n",
      "\n",
      "Epoch 8/30  Batch 55/100 \n",
      "Current Batch Loss 0.29354023933410645\n",
      "\n",
      "Epoch 8/30  Batch 60/100 \n",
      "Current Batch Loss 0.2754497230052948\n",
      "\n",
      "Epoch 8/30  Batch 65/100 \n",
      "Current Batch Loss 0.5042335987091064\n",
      "\n",
      "Epoch 8/30  Batch 70/100 \n",
      "Current Batch Loss 0.3555845022201538\n",
      "\n",
      "Epoch 8/30  Batch 75/100 \n",
      "Current Batch Loss 0.38353610038757324\n",
      "\n",
      "Epoch 8/30  Batch 80/100 \n",
      "Current Batch Loss 0.31947892904281616\n",
      "\n",
      "Epoch 8/30  Batch 85/100 \n",
      "Current Batch Loss 0.3248351812362671\n",
      "\n",
      "Epoch 8/30  Batch 90/100 \n",
      "Current Batch Loss 0.29598042368888855\n",
      "\n",
      "Epoch 8/30  Batch 95/100 \n",
      "Current Batch Loss 0.38712748885154724\n",
      "\n",
      "Epoch 8/30  Batch 100/100 \n",
      "Current Batch Loss 0.4911805987358093\n",
      "\n",
      "==> Epoch 8/30 Epoch Loss 0.012516533955931664\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 9/30  Batch 5/100 \n",
      "Current Batch Loss 0.3261703550815582\n",
      "\n",
      "Epoch 9/30  Batch 10/100 \n",
      "Current Batch Loss 0.2833634316921234\n",
      "\n",
      "Epoch 9/30  Batch 15/100 \n",
      "Current Batch Loss 0.3362157642841339\n",
      "\n",
      "Epoch 9/30  Batch 20/100 \n",
      "Current Batch Loss 0.33155378699302673\n",
      "\n",
      "Epoch 9/30  Batch 25/100 \n",
      "Current Batch Loss 0.297730028629303\n",
      "\n",
      "Epoch 9/30  Batch 30/100 \n",
      "Current Batch Loss 0.37003272771835327\n",
      "\n",
      "Epoch 9/30  Batch 35/100 \n",
      "Current Batch Loss 0.3281106948852539\n",
      "\n",
      "Epoch 9/30  Batch 40/100 \n",
      "Current Batch Loss 0.22707122564315796\n",
      "\n",
      "Epoch 9/30  Batch 45/100 \n",
      "Current Batch Loss 0.30914902687072754\n",
      "\n",
      "Epoch 9/30  Batch 50/100 \n",
      "Current Batch Loss 0.3542642593383789\n",
      "\n",
      "Epoch 9/30  Batch 55/100 \n",
      "Current Batch Loss 0.36737287044525146\n",
      "\n",
      "Epoch 9/30  Batch 60/100 \n",
      "Current Batch Loss 0.31499093770980835\n",
      "\n",
      "Epoch 9/30  Batch 65/100 \n",
      "Current Batch Loss 0.5729548931121826\n",
      "\n",
      "Epoch 9/30  Batch 70/100 \n",
      "Current Batch Loss 0.4283296763896942\n",
      "\n",
      "Epoch 9/30  Batch 75/100 \n",
      "Current Batch Loss 0.3953181505203247\n",
      "\n",
      "Epoch 9/30  Batch 80/100 \n",
      "Current Batch Loss 0.34556227922439575\n",
      "\n",
      "Epoch 9/30  Batch 85/100 \n",
      "Current Batch Loss 0.4451834261417389\n",
      "\n",
      "Epoch 9/30  Batch 90/100 \n",
      "Current Batch Loss 0.27209532260894775\n",
      "\n",
      "Epoch 9/30  Batch 95/100 \n",
      "Current Batch Loss 0.451804518699646\n",
      "\n",
      "Epoch 9/30  Batch 100/100 \n",
      "Current Batch Loss 0.5387638807296753\n",
      "\n",
      "==> Epoch 9/30 Epoch Loss 0.0120355524122715\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 10/30  Batch 5/100 \n",
      "Current Batch Loss 0.3507196009159088\n",
      "\n",
      "Epoch 10/30  Batch 10/100 \n",
      "Current Batch Loss 0.20422616600990295\n",
      "\n",
      "Epoch 10/30  Batch 15/100 \n",
      "Current Batch Loss 0.2635376453399658\n",
      "\n",
      "Epoch 10/30  Batch 20/100 \n",
      "Current Batch Loss 0.26124024391174316\n",
      "\n",
      "Epoch 10/30  Batch 25/100 \n",
      "Current Batch Loss 0.2651829719543457\n",
      "\n",
      "Epoch 10/30  Batch 30/100 \n",
      "Current Batch Loss 0.5020447373390198\n",
      "\n",
      "Epoch 10/30  Batch 35/100 \n",
      "Current Batch Loss 0.44738951325416565\n",
      "\n",
      "Epoch 10/30  Batch 40/100 \n",
      "Current Batch Loss 0.38831228017807007\n",
      "\n",
      "Epoch 10/30  Batch 45/100 \n",
      "Current Batch Loss 0.43592166900634766\n",
      "\n",
      "Epoch 10/30  Batch 50/100 \n",
      "Current Batch Loss 0.3176421523094177\n",
      "\n",
      "Epoch 10/30  Batch 55/100 \n",
      "Current Batch Loss 0.25872012972831726\n",
      "\n",
      "Epoch 10/30  Batch 60/100 \n",
      "Current Batch Loss 0.31494754552841187\n",
      "\n",
      "Epoch 10/30  Batch 65/100 \n",
      "Current Batch Loss 0.5265310406684875\n",
      "\n",
      "Epoch 10/30  Batch 70/100 \n",
      "Current Batch Loss 0.3294185400009155\n",
      "\n",
      "Epoch 10/30  Batch 75/100 \n",
      "Current Batch Loss 0.4301624000072479\n",
      "\n",
      "Epoch 10/30  Batch 80/100 \n",
      "Current Batch Loss 0.42350438237190247\n",
      "\n",
      "Epoch 10/30  Batch 85/100 \n",
      "Current Batch Loss 0.3188452422618866\n",
      "\n",
      "Epoch 10/30  Batch 90/100 \n",
      "Current Batch Loss 0.34117838740348816\n",
      "\n",
      "Epoch 10/30  Batch 95/100 \n",
      "Current Batch Loss 0.49233102798461914\n",
      "\n",
      "Epoch 10/30  Batch 100/100 \n",
      "Current Batch Loss 0.2734597325325012\n",
      "\n",
      "==> Epoch 10/30 Epoch Loss 0.011468329466879368\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 11/30  Batch 5/100 \n",
      "Current Batch Loss 0.34213271737098694\n",
      "\n",
      "Epoch 11/30  Batch 10/100 \n",
      "Current Batch Loss 0.4774344563484192\n",
      "\n",
      "Epoch 11/30  Batch 15/100 \n",
      "Current Batch Loss 0.3311008810997009\n",
      "\n",
      "Epoch 11/30  Batch 20/100 \n",
      "Current Batch Loss 0.2986028492450714\n",
      "\n",
      "Epoch 11/30  Batch 25/100 \n",
      "Current Batch Loss 0.4399128258228302\n",
      "\n",
      "Epoch 11/30  Batch 30/100 \n",
      "Current Batch Loss 0.26295191049575806\n",
      "\n",
      "Epoch 11/30  Batch 35/100 \n",
      "Current Batch Loss 0.30527448654174805\n",
      "\n",
      "Epoch 11/30  Batch 40/100 \n",
      "Current Batch Loss 0.27310821413993835\n",
      "\n",
      "Epoch 11/30  Batch 45/100 \n",
      "Current Batch Loss 0.4474826753139496\n",
      "\n",
      "Epoch 11/30  Batch 50/100 \n",
      "Current Batch Loss 0.2742519676685333\n",
      "\n",
      "Epoch 11/30  Batch 55/100 \n",
      "Current Batch Loss 0.2618900239467621\n",
      "\n",
      "Epoch 11/30  Batch 60/100 \n",
      "Current Batch Loss 0.3489203155040741\n",
      "\n",
      "Epoch 11/30  Batch 65/100 \n",
      "Current Batch Loss 0.2966691851615906\n",
      "\n",
      "Epoch 11/30  Batch 70/100 \n",
      "Current Batch Loss 0.2362390011548996\n",
      "\n",
      "Epoch 11/30  Batch 75/100 \n",
      "Current Batch Loss 0.3304671347141266\n",
      "\n",
      "Epoch 11/30  Batch 80/100 \n",
      "Current Batch Loss 0.24281106889247894\n",
      "\n",
      "Epoch 11/30  Batch 85/100 \n",
      "Current Batch Loss 0.4940379858016968\n",
      "\n",
      "Epoch 11/30  Batch 90/100 \n",
      "Current Batch Loss 0.3264468312263489\n",
      "\n",
      "Epoch 11/30  Batch 95/100 \n",
      "Current Batch Loss 0.2713659405708313\n",
      "\n",
      "Epoch 11/30  Batch 100/100 \n",
      "Current Batch Loss 0.5371400713920593\n",
      "\n",
      "==> Epoch 11/30 Epoch Loss 0.010292409919202328\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 12/30  Batch 5/100 \n",
      "Current Batch Loss 0.3107549846172333\n",
      "\n",
      "Epoch 12/30  Batch 10/100 \n",
      "Current Batch Loss 0.2299329936504364\n",
      "\n",
      "Epoch 12/30  Batch 15/100 \n",
      "Current Batch Loss 0.2050735205411911\n",
      "\n",
      "Epoch 12/30  Batch 20/100 \n",
      "Current Batch Loss 0.33093103766441345\n",
      "\n",
      "Epoch 12/30  Batch 25/100 \n",
      "Current Batch Loss 0.388157457113266\n",
      "\n",
      "Epoch 12/30  Batch 30/100 \n",
      "Current Batch Loss 0.32844626903533936\n",
      "\n",
      "Epoch 12/30  Batch 35/100 \n",
      "Current Batch Loss 0.23937755823135376\n",
      "\n",
      "Epoch 12/30  Batch 40/100 \n",
      "Current Batch Loss 0.22910386323928833\n",
      "\n",
      "Epoch 12/30  Batch 45/100 \n",
      "Current Batch Loss 0.20873290300369263\n",
      "\n",
      "Epoch 12/30  Batch 50/100 \n",
      "Current Batch Loss 0.3553994297981262\n",
      "\n",
      "Epoch 12/30  Batch 55/100 \n",
      "Current Batch Loss 0.3922174572944641\n",
      "\n",
      "Epoch 12/30  Batch 60/100 \n",
      "Current Batch Loss 0.2865174114704132\n",
      "\n",
      "Epoch 12/30  Batch 65/100 \n",
      "Current Batch Loss 0.35662785172462463\n",
      "\n",
      "Epoch 12/30  Batch 70/100 \n",
      "Current Batch Loss 0.3264021575450897\n",
      "\n",
      "Epoch 12/30  Batch 75/100 \n",
      "Current Batch Loss 0.1980152726173401\n",
      "\n",
      "Epoch 12/30  Batch 80/100 \n",
      "Current Batch Loss 0.321185827255249\n",
      "\n",
      "Epoch 12/30  Batch 85/100 \n",
      "Current Batch Loss 0.1670534610748291\n",
      "\n",
      "Epoch 12/30  Batch 90/100 \n",
      "Current Batch Loss 0.2818485200405121\n",
      "\n",
      "Epoch 12/30  Batch 95/100 \n",
      "Current Batch Loss 0.42874017357826233\n",
      "\n",
      "Epoch 12/30  Batch 100/100 \n",
      "Current Batch Loss 0.47528719902038574\n",
      "\n",
      "==> Epoch 12/30 Epoch Loss 0.010454677045345306\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 13/30  Batch 5/100 \n",
      "Current Batch Loss 0.24275663495063782\n",
      "\n",
      "Epoch 13/30  Batch 10/100 \n",
      "Current Batch Loss 0.3368688225746155\n",
      "\n",
      "Epoch 13/30  Batch 15/100 \n",
      "Current Batch Loss 0.27091163396835327\n",
      "\n",
      "Epoch 13/30  Batch 20/100 \n",
      "Current Batch Loss 0.375198632478714\n",
      "\n",
      "Epoch 13/30  Batch 25/100 \n",
      "Current Batch Loss 0.3858077824115753\n",
      "\n",
      "Epoch 13/30  Batch 30/100 \n",
      "Current Batch Loss 0.6754962205886841\n",
      "\n",
      "Epoch 13/30  Batch 35/100 \n",
      "Current Batch Loss 0.24316810071468353\n",
      "\n",
      "Epoch 13/30  Batch 40/100 \n",
      "Current Batch Loss 0.3292136788368225\n",
      "\n",
      "Epoch 13/30  Batch 45/100 \n",
      "Current Batch Loss 0.3260428011417389\n",
      "\n",
      "Epoch 13/30  Batch 50/100 \n",
      "Current Batch Loss 0.3707084059715271\n",
      "\n",
      "Epoch 13/30  Batch 55/100 \n",
      "Current Batch Loss 0.247627392411232\n",
      "\n",
      "Epoch 13/30  Batch 60/100 \n",
      "Current Batch Loss 0.36148664355278015\n",
      "\n",
      "Epoch 13/30  Batch 65/100 \n",
      "Current Batch Loss 0.32655346393585205\n",
      "\n",
      "Epoch 13/30  Batch 70/100 \n",
      "Current Batch Loss 0.401586651802063\n",
      "\n",
      "Epoch 13/30  Batch 75/100 \n",
      "Current Batch Loss 0.2909397780895233\n",
      "\n",
      "Epoch 13/30  Batch 80/100 \n",
      "Current Batch Loss 0.3969798982143402\n",
      "\n",
      "Epoch 13/30  Batch 85/100 \n",
      "Current Batch Loss 0.29701370000839233\n",
      "\n",
      "Epoch 13/30  Batch 90/100 \n",
      "Current Batch Loss 0.36854615807533264\n",
      "\n",
      "Epoch 13/30  Batch 95/100 \n",
      "Current Batch Loss 0.4813002645969391\n",
      "\n",
      "Epoch 13/30  Batch 100/100 \n",
      "Current Batch Loss 0.3110962510108948\n",
      "\n",
      "==> Epoch 13/30 Epoch Loss 0.010564914904534817\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 14/30  Batch 5/100 \n",
      "Current Batch Loss 0.20572733879089355\n",
      "\n",
      "Epoch 14/30  Batch 10/100 \n",
      "Current Batch Loss 0.1957050859928131\n",
      "\n",
      "Epoch 14/30  Batch 15/100 \n",
      "Current Batch Loss 0.35722774267196655\n",
      "\n",
      "Epoch 14/30  Batch 20/100 \n",
      "Current Batch Loss 0.3623313903808594\n",
      "\n",
      "Epoch 14/30  Batch 25/100 \n",
      "Current Batch Loss 0.27659183740615845\n",
      "\n",
      "Epoch 14/30  Batch 30/100 \n",
      "Current Batch Loss 0.2898155450820923\n",
      "\n",
      "Epoch 14/30  Batch 35/100 \n",
      "Current Batch Loss 0.19809873402118683\n",
      "\n",
      "Epoch 14/30  Batch 40/100 \n",
      "Current Batch Loss 0.5863907933235168\n",
      "\n",
      "Epoch 14/30  Batch 45/100 \n",
      "Current Batch Loss 0.22488555312156677\n",
      "\n",
      "Epoch 14/30  Batch 50/100 \n",
      "Current Batch Loss 0.2285425066947937\n",
      "\n",
      "Epoch 14/30  Batch 55/100 \n",
      "Current Batch Loss 0.360580176115036\n",
      "\n",
      "Epoch 14/30  Batch 60/100 \n",
      "Current Batch Loss 0.24973070621490479\n",
      "\n",
      "Epoch 14/30  Batch 65/100 \n",
      "Current Batch Loss 0.17093315720558167\n",
      "\n",
      "Epoch 14/30  Batch 70/100 \n",
      "Current Batch Loss 0.27427437901496887\n",
      "\n",
      "Epoch 14/30  Batch 75/100 \n",
      "Current Batch Loss 0.32984840869903564\n",
      "\n",
      "Epoch 14/30  Batch 80/100 \n",
      "Current Batch Loss 0.3983553647994995\n",
      "\n",
      "Epoch 14/30  Batch 85/100 \n",
      "Current Batch Loss 0.4326123595237732\n",
      "\n",
      "Epoch 14/30  Batch 90/100 \n",
      "Current Batch Loss 0.21105720102787018\n",
      "\n",
      "Epoch 14/30  Batch 95/100 \n",
      "Current Batch Loss 0.24684354662895203\n",
      "\n",
      "Epoch 14/30  Batch 100/100 \n",
      "Current Batch Loss 0.34112268686294556\n",
      "\n",
      "==> Epoch 14/30 Epoch Loss 0.00958214234560728\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 15/30  Batch 5/100 \n",
      "Current Batch Loss 0.18688322603702545\n",
      "\n",
      "Epoch 15/30  Batch 10/100 \n",
      "Current Batch Loss 0.29502490162849426\n",
      "\n",
      "Epoch 15/30  Batch 15/100 \n",
      "Current Batch Loss 0.22198830544948578\n",
      "\n",
      "Epoch 15/30  Batch 20/100 \n",
      "Current Batch Loss 0.38190463185310364\n",
      "\n",
      "Epoch 15/30  Batch 25/100 \n",
      "Current Batch Loss 0.46107593178749084\n",
      "\n",
      "Epoch 15/30  Batch 30/100 \n",
      "Current Batch Loss 0.14625199139118195\n",
      "\n",
      "Epoch 15/30  Batch 35/100 \n",
      "Current Batch Loss 0.33850178122520447\n",
      "\n",
      "Epoch 15/30  Batch 40/100 \n",
      "Current Batch Loss 0.28885337710380554\n",
      "\n",
      "Epoch 15/30  Batch 45/100 \n",
      "Current Batch Loss 0.1852886825799942\n",
      "\n",
      "Epoch 15/30  Batch 50/100 \n",
      "Current Batch Loss 0.2761887013912201\n",
      "\n",
      "Epoch 15/30  Batch 55/100 \n",
      "Current Batch Loss 0.183635413646698\n",
      "\n",
      "Epoch 15/30  Batch 60/100 \n",
      "Current Batch Loss 0.31050756573677063\n",
      "\n",
      "Epoch 15/30  Batch 65/100 \n",
      "Current Batch Loss 0.3122013509273529\n",
      "\n",
      "Epoch 15/30  Batch 70/100 \n",
      "Current Batch Loss 0.22892127931118011\n",
      "\n",
      "Epoch 15/30  Batch 75/100 \n",
      "Current Batch Loss 0.45461103320121765\n",
      "\n",
      "Epoch 15/30  Batch 80/100 \n",
      "Current Batch Loss 0.20440176129341125\n",
      "\n",
      "Epoch 15/30  Batch 85/100 \n",
      "Current Batch Loss 0.28005191683769226\n",
      "\n",
      "Epoch 15/30  Batch 90/100 \n",
      "Current Batch Loss 0.5524715185165405\n",
      "\n",
      "Epoch 15/30  Batch 95/100 \n",
      "Current Batch Loss 0.2749726176261902\n",
      "\n",
      "Epoch 15/30  Batch 100/100 \n",
      "Current Batch Loss 0.49879294633865356\n",
      "\n",
      "==> Epoch 15/30 Epoch Loss 0.009026485495269299\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 16/30  Batch 5/100 \n",
      "Current Batch Loss 0.11258541792631149\n",
      "\n",
      "Epoch 16/30  Batch 10/100 \n",
      "Current Batch Loss 0.3149566650390625\n",
      "\n",
      "Epoch 16/30  Batch 15/100 \n",
      "Current Batch Loss 0.16727498173713684\n",
      "\n",
      "Epoch 16/30  Batch 20/100 \n",
      "Current Batch Loss 0.29782694578170776\n",
      "\n",
      "Epoch 16/30  Batch 25/100 \n",
      "Current Batch Loss 0.1335279941558838\n",
      "\n",
      "Epoch 16/30  Batch 30/100 \n",
      "Current Batch Loss 0.2823040187358856\n",
      "\n",
      "Epoch 16/30  Batch 35/100 \n",
      "Current Batch Loss 0.3027905821800232\n",
      "\n",
      "Epoch 16/30  Batch 40/100 \n",
      "Current Batch Loss 0.28571149706840515\n",
      "\n",
      "Epoch 16/30  Batch 45/100 \n",
      "Current Batch Loss 0.2326081395149231\n",
      "\n",
      "Epoch 16/30  Batch 50/100 \n",
      "Current Batch Loss 0.26401838660240173\n",
      "\n",
      "Epoch 16/30  Batch 55/100 \n",
      "Current Batch Loss 0.3183586597442627\n",
      "\n",
      "Epoch 16/30  Batch 60/100 \n",
      "Current Batch Loss 0.18192008137702942\n",
      "\n",
      "Epoch 16/30  Batch 65/100 \n",
      "Current Batch Loss 0.4412682056427002\n",
      "\n",
      "Epoch 16/30  Batch 70/100 \n",
      "Current Batch Loss 0.27304452657699585\n",
      "\n",
      "Epoch 16/30  Batch 75/100 \n",
      "Current Batch Loss 0.29147809743881226\n",
      "\n",
      "Epoch 16/30  Batch 80/100 \n",
      "Current Batch Loss 0.3300226330757141\n",
      "\n",
      "Epoch 16/30  Batch 85/100 \n",
      "Current Batch Loss 0.22952961921691895\n",
      "\n",
      "Epoch 16/30  Batch 90/100 \n",
      "Current Batch Loss 0.2527298629283905\n",
      "\n",
      "Epoch 16/30  Batch 95/100 \n",
      "Current Batch Loss 0.25281405448913574\n",
      "\n",
      "Epoch 16/30  Batch 100/100 \n",
      "Current Batch Loss 0.5775820016860962\n",
      "\n",
      "==> Epoch 16/30 Epoch Loss 0.008921545930206776\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 17/30  Batch 5/100 \n",
      "Current Batch Loss 0.18164485692977905\n",
      "\n",
      "Epoch 17/30  Batch 10/100 \n",
      "Current Batch Loss 0.34461188316345215\n",
      "\n",
      "Epoch 17/30  Batch 15/100 \n",
      "Current Batch Loss 0.2995729148387909\n",
      "\n",
      "Epoch 17/30  Batch 20/100 \n",
      "Current Batch Loss 0.29103726148605347\n",
      "\n",
      "Epoch 17/30  Batch 25/100 \n",
      "Current Batch Loss 0.26092424988746643\n",
      "\n",
      "Epoch 17/30  Batch 30/100 \n",
      "Current Batch Loss 0.16436389088630676\n",
      "\n",
      "Epoch 17/30  Batch 35/100 \n",
      "Current Batch Loss 0.19574101269245148\n",
      "\n",
      "Epoch 17/30  Batch 40/100 \n",
      "Current Batch Loss 0.16106797754764557\n",
      "\n",
      "Epoch 17/30  Batch 45/100 \n",
      "Current Batch Loss 0.23175294697284698\n",
      "\n",
      "Epoch 17/30  Batch 50/100 \n",
      "Current Batch Loss 0.3092518448829651\n",
      "\n",
      "Epoch 17/30  Batch 55/100 \n",
      "Current Batch Loss 0.3455434739589691\n",
      "\n",
      "Epoch 17/30  Batch 60/100 \n",
      "Current Batch Loss 0.32736629247665405\n",
      "\n",
      "Epoch 17/30  Batch 65/100 \n",
      "Current Batch Loss 0.3483523726463318\n",
      "\n",
      "Epoch 17/30  Batch 70/100 \n",
      "Current Batch Loss 0.20548056066036224\n",
      "\n",
      "Epoch 17/30  Batch 75/100 \n",
      "Current Batch Loss 0.28022053837776184\n",
      "\n",
      "Epoch 17/30  Batch 80/100 \n",
      "Current Batch Loss 0.2747313678264618\n",
      "\n",
      "Epoch 17/30  Batch 85/100 \n",
      "Current Batch Loss 0.40210750699043274\n",
      "\n",
      "Epoch 17/30  Batch 90/100 \n",
      "Current Batch Loss 0.2108059674501419\n",
      "\n",
      "Epoch 17/30  Batch 95/100 \n",
      "Current Batch Loss 0.2502703368663788\n",
      "\n",
      "Epoch 17/30  Batch 100/100 \n",
      "Current Batch Loss 0.2548130452632904\n",
      "\n",
      "==> Epoch 17/30 Epoch Loss 0.008505691774189472\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 18/30  Batch 5/100 \n",
      "Current Batch Loss 0.3968888819217682\n",
      "\n",
      "Epoch 18/30  Batch 10/100 \n",
      "Current Batch Loss 0.2586481273174286\n",
      "\n",
      "Epoch 18/30  Batch 15/100 \n",
      "Current Batch Loss 0.19929605722427368\n",
      "\n",
      "Epoch 18/30  Batch 20/100 \n",
      "Current Batch Loss 0.3278502821922302\n",
      "\n",
      "Epoch 18/30  Batch 25/100 \n",
      "Current Batch Loss 0.22876165807247162\n",
      "\n",
      "Epoch 18/30  Batch 30/100 \n",
      "Current Batch Loss 0.2674415409564972\n",
      "\n",
      "Epoch 18/30  Batch 35/100 \n",
      "Current Batch Loss 0.16779352724552155\n",
      "\n",
      "Epoch 18/30  Batch 40/100 \n",
      "Current Batch Loss 0.2068178951740265\n",
      "\n",
      "Epoch 18/30  Batch 45/100 \n",
      "Current Batch Loss 0.33681225776672363\n",
      "\n",
      "Epoch 18/30  Batch 50/100 \n",
      "Current Batch Loss 0.318112850189209\n",
      "\n",
      "Epoch 18/30  Batch 55/100 \n",
      "Current Batch Loss 0.16463503241539001\n",
      "\n",
      "Epoch 18/30  Batch 60/100 \n",
      "Current Batch Loss 0.25338003039360046\n",
      "\n",
      "Epoch 18/30  Batch 65/100 \n",
      "Current Batch Loss 0.2299272119998932\n",
      "\n",
      "Epoch 18/30  Batch 70/100 \n",
      "Current Batch Loss 0.15028515458106995\n",
      "\n",
      "Epoch 18/30  Batch 75/100 \n",
      "Current Batch Loss 0.21672557294368744\n",
      "\n",
      "Epoch 18/30  Batch 80/100 \n",
      "Current Batch Loss 0.2536093294620514\n",
      "\n",
      "Epoch 18/30  Batch 85/100 \n",
      "Current Batch Loss 0.23991626501083374\n",
      "\n",
      "Epoch 18/30  Batch 90/100 \n",
      "Current Batch Loss 0.24966642260551453\n",
      "\n",
      "Epoch 18/30  Batch 95/100 \n",
      "Current Batch Loss 0.29733163118362427\n",
      "\n",
      "Epoch 18/30  Batch 100/100 \n",
      "Current Batch Loss 0.22693227231502533\n",
      "\n",
      "==> Epoch 18/30 Epoch Loss 0.00839352235198021\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 19/30  Batch 5/100 \n",
      "Current Batch Loss 0.08395728468894958\n",
      "\n",
      "Epoch 19/30  Batch 10/100 \n",
      "Current Batch Loss 0.10213756561279297\n",
      "\n",
      "Epoch 19/30  Batch 15/100 \n",
      "Current Batch Loss 0.1697075366973877\n",
      "\n",
      "Epoch 19/30  Batch 20/100 \n",
      "Current Batch Loss 0.36916252970695496\n",
      "\n",
      "Epoch 19/30  Batch 25/100 \n",
      "Current Batch Loss 0.35421523451805115\n",
      "\n",
      "Epoch 19/30  Batch 30/100 \n",
      "Current Batch Loss 0.21396282315254211\n",
      "\n",
      "Epoch 19/30  Batch 35/100 \n",
      "Current Batch Loss 0.3665909469127655\n",
      "\n",
      "Epoch 19/30  Batch 40/100 \n",
      "Current Batch Loss 0.1826932281255722\n",
      "\n",
      "Epoch 19/30  Batch 45/100 \n",
      "Current Batch Loss 0.20891417562961578\n",
      "\n",
      "Epoch 19/30  Batch 50/100 \n",
      "Current Batch Loss 0.3386448919773102\n",
      "\n",
      "Epoch 19/30  Batch 55/100 \n",
      "Current Batch Loss 0.3328445851802826\n",
      "\n",
      "Epoch 19/30  Batch 60/100 \n",
      "Current Batch Loss 0.24936726689338684\n",
      "\n",
      "Epoch 19/30  Batch 65/100 \n",
      "Current Batch Loss 0.16039495170116425\n",
      "\n",
      "Epoch 19/30  Batch 70/100 \n",
      "Current Batch Loss 0.380479097366333\n",
      "\n",
      "Epoch 19/30  Batch 75/100 \n",
      "Current Batch Loss 0.5091279745101929\n",
      "\n",
      "Epoch 19/30  Batch 80/100 \n",
      "Current Batch Loss 0.20647139847278595\n",
      "\n",
      "Epoch 19/30  Batch 85/100 \n",
      "Current Batch Loss 0.28911128640174866\n",
      "\n",
      "Epoch 19/30  Batch 90/100 \n",
      "Current Batch Loss 0.21767272055149078\n",
      "\n",
      "Epoch 19/30  Batch 95/100 \n",
      "Current Batch Loss 0.28340351581573486\n",
      "\n",
      "Epoch 19/30  Batch 100/100 \n",
      "Current Batch Loss 0.3349246382713318\n",
      "\n",
      "==> Epoch 19/30 Epoch Loss 0.008534472435712814\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 20/30  Batch 5/100 \n",
      "Current Batch Loss 0.14027629792690277\n",
      "\n",
      "Epoch 20/30  Batch 10/100 \n",
      "Current Batch Loss 0.1486036628484726\n",
      "\n",
      "Epoch 20/30  Batch 15/100 \n",
      "Current Batch Loss 0.3953511416912079\n",
      "\n",
      "Epoch 20/30  Batch 20/100 \n",
      "Current Batch Loss 0.317141592502594\n",
      "\n",
      "Epoch 20/30  Batch 25/100 \n",
      "Current Batch Loss 0.20441539585590363\n",
      "\n",
      "Epoch 20/30  Batch 30/100 \n",
      "Current Batch Loss 0.15103021264076233\n",
      "\n",
      "Epoch 20/30  Batch 35/100 \n",
      "Current Batch Loss 0.5216993093490601\n",
      "\n",
      "Epoch 20/30  Batch 40/100 \n",
      "Current Batch Loss 0.17133627831935883\n",
      "\n",
      "Epoch 20/30  Batch 45/100 \n",
      "Current Batch Loss 0.3845074772834778\n",
      "\n",
      "Epoch 20/30  Batch 50/100 \n",
      "Current Batch Loss 0.25519123673439026\n",
      "\n",
      "Epoch 20/30  Batch 55/100 \n",
      "Current Batch Loss 0.12233789265155792\n",
      "\n",
      "Epoch 20/30  Batch 60/100 \n",
      "Current Batch Loss 0.15152019262313843\n",
      "\n",
      "Epoch 20/30  Batch 65/100 \n",
      "Current Batch Loss 0.18683549761772156\n",
      "\n",
      "Epoch 20/30  Batch 70/100 \n",
      "Current Batch Loss 0.1553734689950943\n",
      "\n",
      "Epoch 20/30  Batch 75/100 \n",
      "Current Batch Loss 0.5313677787780762\n",
      "\n",
      "Epoch 20/30  Batch 80/100 \n",
      "Current Batch Loss 0.20817068219184875\n",
      "\n",
      "Epoch 20/30  Batch 85/100 \n",
      "Current Batch Loss 0.2810267508029938\n",
      "\n",
      "Epoch 20/30  Batch 90/100 \n",
      "Current Batch Loss 0.3712156414985657\n",
      "\n",
      "Epoch 20/30  Batch 95/100 \n",
      "Current Batch Loss 0.5068625211715698\n",
      "\n",
      "Epoch 20/30  Batch 100/100 \n",
      "Current Batch Loss 0.209238201379776\n",
      "\n",
      "==> Epoch 20/30 Epoch Loss 0.008216631598770618\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 21/30  Batch 5/100 \n",
      "Current Batch Loss 0.2156713902950287\n",
      "\n",
      "Epoch 21/30  Batch 10/100 \n",
      "Current Batch Loss 0.17905019223690033\n",
      "\n",
      "Epoch 21/30  Batch 15/100 \n",
      "Current Batch Loss 0.20277488231658936\n",
      "\n",
      "Epoch 21/30  Batch 20/100 \n",
      "Current Batch Loss 0.12435878813266754\n",
      "\n",
      "Epoch 21/30  Batch 25/100 \n",
      "Current Batch Loss 0.30757102370262146\n",
      "\n",
      "Epoch 21/30  Batch 30/100 \n",
      "Current Batch Loss 0.2658878266811371\n",
      "\n",
      "Epoch 21/30  Batch 35/100 \n",
      "Current Batch Loss 0.2839607000350952\n",
      "\n",
      "Epoch 21/30  Batch 40/100 \n",
      "Current Batch Loss 0.24234122037887573\n",
      "\n",
      "Epoch 21/30  Batch 45/100 \n",
      "Current Batch Loss 0.1709262728691101\n",
      "\n",
      "Epoch 21/30  Batch 50/100 \n",
      "Current Batch Loss 0.16451649367809296\n",
      "\n",
      "Epoch 21/30  Batch 55/100 \n",
      "Current Batch Loss 0.34057343006134033\n",
      "\n",
      "Epoch 21/30  Batch 60/100 \n",
      "Current Batch Loss 0.2719186842441559\n",
      "\n",
      "Epoch 21/30  Batch 65/100 \n",
      "Current Batch Loss 0.26500844955444336\n",
      "\n",
      "Epoch 21/30  Batch 70/100 \n",
      "Current Batch Loss 0.1337200403213501\n",
      "\n",
      "Epoch 21/30  Batch 75/100 \n",
      "Current Batch Loss 0.34359756112098694\n",
      "\n",
      "Epoch 21/30  Batch 80/100 \n",
      "Current Batch Loss 0.2583886981010437\n",
      "\n",
      "Epoch 21/30  Batch 85/100 \n",
      "Current Batch Loss 0.17136690020561218\n",
      "\n",
      "Epoch 21/30  Batch 90/100 \n",
      "Current Batch Loss 0.21826188266277313\n",
      "\n",
      "Epoch 21/30  Batch 95/100 \n",
      "Current Batch Loss 0.2753019630908966\n",
      "\n",
      "Epoch 21/30  Batch 100/100 \n",
      "Current Batch Loss 0.3888612389564514\n",
      "\n",
      "==> Epoch 21/30 Epoch Loss 0.007825605571269989\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 22/30  Batch 5/100 \n",
      "Current Batch Loss 0.400398850440979\n",
      "\n",
      "Epoch 22/30  Batch 10/100 \n",
      "Current Batch Loss 0.2055269032716751\n",
      "\n",
      "Epoch 22/30  Batch 15/100 \n",
      "Current Batch Loss 0.24688363075256348\n",
      "\n",
      "Epoch 22/30  Batch 20/100 \n",
      "Current Batch Loss 0.26253026723861694\n",
      "\n",
      "Epoch 22/30  Batch 25/100 \n",
      "Current Batch Loss 0.25372737646102905\n",
      "\n",
      "Epoch 22/30  Batch 30/100 \n",
      "Current Batch Loss 0.2364199161529541\n",
      "\n",
      "Epoch 22/30  Batch 35/100 \n",
      "Current Batch Loss 0.3265232741832733\n",
      "\n",
      "Epoch 22/30  Batch 40/100 \n",
      "Current Batch Loss 0.2874886691570282\n",
      "\n",
      "Epoch 22/30  Batch 45/100 \n",
      "Current Batch Loss 0.11891607940196991\n",
      "\n",
      "Epoch 22/30  Batch 50/100 \n",
      "Current Batch Loss 0.29634231328964233\n",
      "\n",
      "Epoch 22/30  Batch 55/100 \n",
      "Current Batch Loss 0.22712983191013336\n",
      "\n",
      "Epoch 22/30  Batch 60/100 \n",
      "Current Batch Loss 0.19275206327438354\n",
      "\n",
      "Epoch 22/30  Batch 65/100 \n",
      "Current Batch Loss 0.2464837282896042\n",
      "\n",
      "Epoch 22/30  Batch 70/100 \n",
      "Current Batch Loss 0.16811242699623108\n",
      "\n",
      "Epoch 22/30  Batch 75/100 \n",
      "Current Batch Loss 0.20138436555862427\n",
      "\n",
      "Epoch 22/30  Batch 80/100 \n",
      "Current Batch Loss 0.16139711439609528\n",
      "\n",
      "Epoch 22/30  Batch 85/100 \n",
      "Current Batch Loss 0.24658706784248352\n",
      "\n",
      "Epoch 22/30  Batch 90/100 \n",
      "Current Batch Loss 0.26274269819259644\n",
      "\n",
      "Epoch 22/30  Batch 95/100 \n",
      "Current Batch Loss 0.13424097001552582\n",
      "\n",
      "Epoch 22/30  Batch 100/100 \n",
      "Current Batch Loss 0.1754634976387024\n",
      "\n",
      "==> Epoch 22/30 Epoch Loss 0.007458028383553028\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 23/30  Batch 5/100 \n",
      "Current Batch Loss 0.2565145790576935\n",
      "\n",
      "Epoch 23/30  Batch 10/100 \n",
      "Current Batch Loss 0.16194738447666168\n",
      "\n",
      "Epoch 23/30  Batch 15/100 \n",
      "Current Batch Loss 0.18611784279346466\n",
      "\n",
      "Epoch 23/30  Batch 20/100 \n",
      "Current Batch Loss 0.09198827296495438\n",
      "\n",
      "Epoch 23/30  Batch 25/100 \n",
      "Current Batch Loss 0.16422101855278015\n",
      "\n",
      "Epoch 23/30  Batch 30/100 \n",
      "Current Batch Loss 0.20527657866477966\n",
      "\n",
      "Epoch 23/30  Batch 35/100 \n",
      "Current Batch Loss 0.10894157737493515\n",
      "\n",
      "Epoch 23/30  Batch 40/100 \n",
      "Current Batch Loss 0.24597994983196259\n",
      "\n",
      "Epoch 23/30  Batch 45/100 \n",
      "Current Batch Loss 0.15085335075855255\n",
      "\n",
      "Epoch 23/30  Batch 50/100 \n",
      "Current Batch Loss 0.19785083830356598\n",
      "\n",
      "Epoch 23/30  Batch 55/100 \n",
      "Current Batch Loss 0.1638639271259308\n",
      "\n",
      "Epoch 23/30  Batch 60/100 \n",
      "Current Batch Loss 0.26601555943489075\n",
      "\n",
      "Epoch 23/30  Batch 65/100 \n",
      "Current Batch Loss 0.22110827267169952\n",
      "\n",
      "Epoch 23/30  Batch 70/100 \n",
      "Current Batch Loss 0.1655360758304596\n",
      "\n",
      "Epoch 23/30  Batch 75/100 \n",
      "Current Batch Loss 0.2873046100139618\n",
      "\n",
      "Epoch 23/30  Batch 80/100 \n",
      "Current Batch Loss 0.26472413539886475\n",
      "\n",
      "Epoch 23/30  Batch 85/100 \n",
      "Current Batch Loss 0.20922064781188965\n",
      "\n",
      "Epoch 23/30  Batch 90/100 \n",
      "Current Batch Loss 0.19651111960411072\n",
      "\n",
      "Epoch 23/30  Batch 95/100 \n",
      "Current Batch Loss 0.14934173226356506\n",
      "\n",
      "Epoch 23/30  Batch 100/100 \n",
      "Current Batch Loss 0.24570289254188538\n",
      "\n",
      "==> Epoch 23/30 Epoch Loss 0.007160770706832409\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 24/30  Batch 5/100 \n",
      "Current Batch Loss 0.27933382987976074\n",
      "\n",
      "Epoch 24/30  Batch 10/100 \n",
      "Current Batch Loss 0.10409300029277802\n",
      "\n",
      "Epoch 24/30  Batch 15/100 \n",
      "Current Batch Loss 0.09151681512594223\n",
      "\n",
      "Epoch 24/30  Batch 20/100 \n",
      "Current Batch Loss 0.1553439199924469\n",
      "\n",
      "Epoch 24/30  Batch 25/100 \n",
      "Current Batch Loss 0.12756024301052094\n",
      "\n",
      "Epoch 24/30  Batch 30/100 \n",
      "Current Batch Loss 0.22625170648097992\n",
      "\n",
      "Epoch 24/30  Batch 35/100 \n",
      "Current Batch Loss 0.2822169065475464\n",
      "\n",
      "Epoch 24/30  Batch 40/100 \n",
      "Current Batch Loss 0.2789683938026428\n",
      "\n",
      "Epoch 24/30  Batch 45/100 \n",
      "Current Batch Loss 0.24677860736846924\n",
      "\n",
      "Epoch 24/30  Batch 50/100 \n",
      "Current Batch Loss 0.28673404455184937\n",
      "\n",
      "Epoch 24/30  Batch 55/100 \n",
      "Current Batch Loss 0.1415257453918457\n",
      "\n",
      "Epoch 24/30  Batch 60/100 \n",
      "Current Batch Loss 0.19499242305755615\n",
      "\n",
      "Epoch 24/30  Batch 65/100 \n",
      "Current Batch Loss 0.3582824766635895\n",
      "\n",
      "Epoch 24/30  Batch 70/100 \n",
      "Current Batch Loss 0.07613176852464676\n",
      "\n",
      "Epoch 24/30  Batch 75/100 \n",
      "Current Batch Loss 0.3664741516113281\n",
      "\n",
      "Epoch 24/30  Batch 80/100 \n",
      "Current Batch Loss 0.36220821738243103\n",
      "\n",
      "Epoch 24/30  Batch 85/100 \n",
      "Current Batch Loss 0.2758355438709259\n",
      "\n",
      "Epoch 24/30  Batch 90/100 \n",
      "Current Batch Loss 0.33283713459968567\n",
      "\n",
      "Epoch 24/30  Batch 95/100 \n",
      "Current Batch Loss 0.1981462836265564\n",
      "\n",
      "Epoch 24/30  Batch 100/100 \n",
      "Current Batch Loss 0.511692225933075\n",
      "\n",
      "==> Epoch 24/30 Epoch Loss 0.007718909531831741\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 25/30  Batch 5/100 \n",
      "Current Batch Loss 0.26139262318611145\n",
      "\n",
      "Epoch 25/30  Batch 10/100 \n",
      "Current Batch Loss 0.23977158963680267\n",
      "\n",
      "Epoch 25/30  Batch 15/100 \n",
      "Current Batch Loss 0.3700605034828186\n",
      "\n",
      "Epoch 25/30  Batch 20/100 \n",
      "Current Batch Loss 0.2214086651802063\n",
      "\n",
      "Epoch 25/30  Batch 25/100 \n",
      "Current Batch Loss 0.28928300738334656\n",
      "\n",
      "Epoch 25/30  Batch 30/100 \n",
      "Current Batch Loss 0.34074684977531433\n",
      "\n",
      "Epoch 25/30  Batch 35/100 \n",
      "Current Batch Loss 0.2120993286371231\n",
      "\n",
      "Epoch 25/30  Batch 40/100 \n",
      "Current Batch Loss 0.1428859680891037\n",
      "\n",
      "Epoch 25/30  Batch 45/100 \n",
      "Current Batch Loss 0.15454654395580292\n",
      "\n",
      "Epoch 25/30  Batch 50/100 \n",
      "Current Batch Loss 0.29937469959259033\n",
      "\n",
      "Epoch 25/30  Batch 55/100 \n",
      "Current Batch Loss 0.1669481247663498\n",
      "\n",
      "Epoch 25/30  Batch 60/100 \n",
      "Current Batch Loss 0.30289310216903687\n",
      "\n",
      "Epoch 25/30  Batch 65/100 \n",
      "Current Batch Loss 0.18047687411308289\n",
      "\n",
      "Epoch 25/30  Batch 70/100 \n",
      "Current Batch Loss 0.253301739692688\n",
      "\n",
      "Epoch 25/30  Batch 75/100 \n",
      "Current Batch Loss 0.25256484746932983\n",
      "\n",
      "Epoch 25/30  Batch 80/100 \n",
      "Current Batch Loss 0.18346884846687317\n",
      "\n",
      "Epoch 25/30  Batch 85/100 \n",
      "Current Batch Loss 0.2555493116378784\n",
      "\n",
      "Epoch 25/30  Batch 90/100 \n",
      "Current Batch Loss 0.20333456993103027\n",
      "\n",
      "Epoch 25/30  Batch 95/100 \n",
      "Current Batch Loss 0.32564157247543335\n",
      "\n",
      "Epoch 25/30  Batch 100/100 \n",
      "Current Batch Loss 0.28868940472602844\n",
      "\n",
      "==> Epoch 25/30 Epoch Loss 0.007897881790995598\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 26/30  Batch 5/100 \n",
      "Current Batch Loss 0.13796517252922058\n",
      "\n",
      "Epoch 26/30  Batch 10/100 \n",
      "Current Batch Loss 0.26117873191833496\n",
      "\n",
      "Epoch 26/30  Batch 15/100 \n",
      "Current Batch Loss 0.24490204453468323\n",
      "\n",
      "Epoch 26/30  Batch 20/100 \n",
      "Current Batch Loss 0.09195435047149658\n",
      "\n",
      "Epoch 26/30  Batch 25/100 \n",
      "Current Batch Loss 0.060379158705472946\n",
      "\n",
      "Epoch 26/30  Batch 30/100 \n",
      "Current Batch Loss 0.3645845949649811\n",
      "\n",
      "Epoch 26/30  Batch 35/100 \n",
      "Current Batch Loss 0.22281600534915924\n",
      "\n",
      "Epoch 26/30  Batch 40/100 \n",
      "Current Batch Loss 0.24444153904914856\n",
      "\n",
      "Epoch 26/30  Batch 45/100 \n",
      "Current Batch Loss 0.2745351791381836\n",
      "\n",
      "Epoch 26/30  Batch 50/100 \n",
      "Current Batch Loss 0.17510071396827698\n",
      "\n",
      "Epoch 26/30  Batch 55/100 \n",
      "Current Batch Loss 0.27882009744644165\n",
      "\n",
      "Epoch 26/30  Batch 60/100 \n",
      "Current Batch Loss 0.35181406140327454\n",
      "\n",
      "Epoch 26/30  Batch 65/100 \n",
      "Current Batch Loss 0.2247888445854187\n",
      "\n",
      "Epoch 26/30  Batch 70/100 \n",
      "Current Batch Loss 0.21044288575649261\n",
      "\n",
      "Epoch 26/30  Batch 75/100 \n",
      "Current Batch Loss 0.2711508870124817\n",
      "\n",
      "Epoch 26/30  Batch 80/100 \n",
      "Current Batch Loss 0.12127532064914703\n",
      "\n",
      "Epoch 26/30  Batch 85/100 \n",
      "Current Batch Loss 0.16681408882141113\n",
      "\n",
      "Epoch 26/30  Batch 90/100 \n",
      "Current Batch Loss 0.2378416806459427\n",
      "\n",
      "Epoch 26/30  Batch 95/100 \n",
      "Current Batch Loss 0.1745678186416626\n",
      "\n",
      "Epoch 26/30  Batch 100/100 \n",
      "Current Batch Loss 0.1652860939502716\n",
      "\n",
      "==> Epoch 26/30 Epoch Loss 0.006960536353290081\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 27/30  Batch 5/100 \n",
      "Current Batch Loss 0.23127400875091553\n",
      "\n",
      "Epoch 27/30  Batch 10/100 \n",
      "Current Batch Loss 0.16911447048187256\n",
      "\n",
      "Epoch 27/30  Batch 15/100 \n",
      "Current Batch Loss 0.37216803431510925\n",
      "\n",
      "Epoch 27/30  Batch 20/100 \n",
      "Current Batch Loss 0.20124641060829163\n",
      "\n",
      "Epoch 27/30  Batch 25/100 \n",
      "Current Batch Loss 0.23284780979156494\n",
      "\n",
      "Epoch 27/30  Batch 30/100 \n",
      "Current Batch Loss 0.20139023661613464\n",
      "\n",
      "Epoch 27/30  Batch 35/100 \n",
      "Current Batch Loss 0.13179132342338562\n",
      "\n",
      "Epoch 27/30  Batch 40/100 \n",
      "Current Batch Loss 0.1646995097398758\n",
      "\n",
      "Epoch 27/30  Batch 45/100 \n",
      "Current Batch Loss 0.18837392330169678\n",
      "\n",
      "Epoch 27/30  Batch 50/100 \n",
      "Current Batch Loss 0.12642692029476166\n",
      "\n",
      "Epoch 27/30  Batch 55/100 \n",
      "Current Batch Loss 0.2738679349422455\n",
      "\n",
      "Epoch 27/30  Batch 60/100 \n",
      "Current Batch Loss 0.37365180253982544\n",
      "\n",
      "Epoch 27/30  Batch 65/100 \n",
      "Current Batch Loss 0.3541735112667084\n",
      "\n",
      "Epoch 27/30  Batch 70/100 \n",
      "Current Batch Loss 0.17945465445518494\n",
      "\n",
      "Epoch 27/30  Batch 75/100 \n",
      "Current Batch Loss 0.3715273141860962\n",
      "\n",
      "Epoch 27/30  Batch 80/100 \n",
      "Current Batch Loss 0.2761465609073639\n",
      "\n",
      "Epoch 27/30  Batch 85/100 \n",
      "Current Batch Loss 0.1433604210615158\n",
      "\n",
      "Epoch 27/30  Batch 90/100 \n",
      "Current Batch Loss 0.2504947781562805\n",
      "\n",
      "Epoch 27/30  Batch 95/100 \n",
      "Current Batch Loss 0.15123215317726135\n",
      "\n",
      "Epoch 27/30  Batch 100/100 \n",
      "Current Batch Loss 0.18498681485652924\n",
      "\n",
      "==> Epoch 27/30 Epoch Loss 0.007140038069337606\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 28/30  Batch 5/100 \n",
      "Current Batch Loss 0.3097081184387207\n",
      "\n",
      "Epoch 28/30  Batch 10/100 \n",
      "Current Batch Loss 0.21042083203792572\n",
      "\n",
      "Epoch 28/30  Batch 15/100 \n",
      "Current Batch Loss 0.36715584993362427\n",
      "\n",
      "Epoch 28/30  Batch 20/100 \n",
      "Current Batch Loss 0.13004563748836517\n",
      "\n",
      "Epoch 28/30  Batch 25/100 \n",
      "Current Batch Loss 0.1543787270784378\n",
      "\n",
      "Epoch 28/30  Batch 30/100 \n",
      "Current Batch Loss 0.19842538237571716\n",
      "\n",
      "Epoch 28/30  Batch 35/100 \n",
      "Current Batch Loss 0.2374519407749176\n",
      "\n",
      "Epoch 28/30  Batch 40/100 \n",
      "Current Batch Loss 0.14911004900932312\n",
      "\n",
      "Epoch 28/30  Batch 45/100 \n",
      "Current Batch Loss 0.3045305609703064\n",
      "\n",
      "Epoch 28/30  Batch 50/100 \n",
      "Current Batch Loss 0.1563023030757904\n",
      "\n",
      "Epoch 28/30  Batch 55/100 \n",
      "Current Batch Loss 0.13463141024112701\n",
      "\n",
      "Epoch 28/30  Batch 60/100 \n",
      "Current Batch Loss 0.20265057682991028\n",
      "\n",
      "Epoch 28/30  Batch 65/100 \n",
      "Current Batch Loss 0.17438143491744995\n",
      "\n",
      "Epoch 28/30  Batch 70/100 \n",
      "Current Batch Loss 0.2956133484840393\n",
      "\n",
      "Epoch 28/30  Batch 75/100 \n",
      "Current Batch Loss 0.2722204327583313\n",
      "\n",
      "Epoch 28/30  Batch 80/100 \n",
      "Current Batch Loss 0.45820876955986023\n",
      "\n",
      "Epoch 28/30  Batch 85/100 \n",
      "Current Batch Loss 0.15947175025939941\n",
      "\n",
      "Epoch 28/30  Batch 90/100 \n",
      "Current Batch Loss 0.17382587492465973\n",
      "\n",
      "Epoch 28/30  Batch 95/100 \n",
      "Current Batch Loss 0.17549408972263336\n",
      "\n",
      "Epoch 28/30  Batch 100/100 \n",
      "Current Batch Loss 0.2584717571735382\n",
      "\n",
      "==> Epoch 28/30 Epoch Loss 0.007100569549947977\n",
      "### Epoch Loss did not improve\n",
      "\n",
      "Epoch 29/30  Batch 5/100 \n",
      "Current Batch Loss 0.231536403298378\n",
      "\n",
      "Epoch 29/30  Batch 10/100 \n",
      "Current Batch Loss 0.3229031562805176\n",
      "\n",
      "Epoch 29/30  Batch 15/100 \n",
      "Current Batch Loss 0.043376460671424866\n",
      "\n",
      "Epoch 29/30  Batch 20/100 \n",
      "Current Batch Loss 0.07177014648914337\n",
      "\n",
      "Epoch 29/30  Batch 25/100 \n",
      "Current Batch Loss 0.12470510601997375\n",
      "\n",
      "Epoch 29/30  Batch 30/100 \n",
      "Current Batch Loss 0.12268976122140884\n",
      "\n",
      "Epoch 29/30  Batch 35/100 \n",
      "Current Batch Loss 0.20139051973819733\n",
      "\n",
      "Epoch 29/30  Batch 40/100 \n",
      "Current Batch Loss 0.22632284462451935\n",
      "\n",
      "Epoch 29/30  Batch 45/100 \n",
      "Current Batch Loss 0.1979629248380661\n",
      "\n",
      "Epoch 29/30  Batch 50/100 \n",
      "Current Batch Loss 0.20506715774536133\n",
      "\n",
      "Epoch 29/30  Batch 55/100 \n",
      "Current Batch Loss 0.22436168789863586\n",
      "\n",
      "Epoch 29/30  Batch 60/100 \n",
      "Current Batch Loss 0.14341792464256287\n",
      "\n",
      "Epoch 29/30  Batch 65/100 \n",
      "Current Batch Loss 0.2623629868030548\n",
      "\n",
      "Epoch 29/30  Batch 70/100 \n",
      "Current Batch Loss 0.22091609239578247\n",
      "\n",
      "Epoch 29/30  Batch 75/100 \n",
      "Current Batch Loss 0.2253570854663849\n",
      "\n",
      "Epoch 29/30  Batch 80/100 \n",
      "Current Batch Loss 0.3124867081642151\n",
      "\n",
      "Epoch 29/30  Batch 85/100 \n",
      "Current Batch Loss 0.22342127561569214\n",
      "\n",
      "Epoch 29/30  Batch 90/100 \n",
      "Current Batch Loss 0.13580924272537231\n",
      "\n",
      "Epoch 29/30  Batch 95/100 \n",
      "Current Batch Loss 0.24758099019527435\n",
      "\n",
      "Epoch 29/30  Batch 100/100 \n",
      "Current Batch Loss 0.45277029275894165\n",
      "\n",
      "==> Epoch 29/30 Epoch Loss 0.006890598684549332\n",
      "$$$ Saved a new checkpoint\n",
      "\n",
      "Epoch 30/30  Batch 5/100 \n",
      "Current Batch Loss 0.282034307718277\n",
      "\n",
      "Epoch 30/30  Batch 10/100 \n",
      "Current Batch Loss 0.19153574109077454\n",
      "\n",
      "Epoch 30/30  Batch 15/100 \n",
      "Current Batch Loss 0.2094164788722992\n",
      "\n",
      "Epoch 30/30  Batch 20/100 \n",
      "Current Batch Loss 0.20716524124145508\n",
      "\n",
      "Epoch 30/30  Batch 25/100 \n",
      "Current Batch Loss 0.26161637902259827\n",
      "\n",
      "Epoch 30/30  Batch 30/100 \n",
      "Current Batch Loss 0.09897808730602264\n",
      "\n",
      "Epoch 30/30  Batch 35/100 \n",
      "Current Batch Loss 0.29093360900878906\n",
      "\n",
      "Epoch 30/30  Batch 40/100 \n",
      "Current Batch Loss 0.11770560592412949\n",
      "\n",
      "Epoch 30/30  Batch 45/100 \n",
      "Current Batch Loss 0.25578275322914124\n",
      "\n",
      "Epoch 30/30  Batch 50/100 \n",
      "Current Batch Loss 0.1036049947142601\n",
      "\n",
      "Epoch 30/30  Batch 55/100 \n",
      "Current Batch Loss 0.29860034584999084\n",
      "\n",
      "Epoch 30/30  Batch 60/100 \n",
      "Current Batch Loss 0.22847871482372284\n",
      "\n",
      "Epoch 30/30  Batch 65/100 \n",
      "Current Batch Loss 0.19044229388237\n",
      "\n",
      "Epoch 30/30  Batch 70/100 \n",
      "Current Batch Loss 0.24168695509433746\n",
      "\n",
      "Epoch 30/30  Batch 75/100 \n",
      "Current Batch Loss 0.28366658091545105\n",
      "\n",
      "Epoch 30/30  Batch 80/100 \n",
      "Current Batch Loss 0.2558005750179291\n",
      "\n",
      "Epoch 30/30  Batch 85/100 \n",
      "Current Batch Loss 0.09990556538105011\n",
      "\n",
      "Epoch 30/30  Batch 90/100 \n",
      "Current Batch Loss 0.18757587671279907\n",
      "\n",
      "Epoch 30/30  Batch 95/100 \n",
      "Current Batch Loss 0.34094762802124023\n",
      "\n",
      "Epoch 30/30  Batch 100/100 \n",
      "Current Batch Loss 0.14595749974250793\n",
      "\n",
      "==> Epoch 30/30 Epoch Loss 0.007127867545932531\n",
      "### Epoch Loss did not improve\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    running_loss = torch.zeros(1)\n",
    "    \n",
    "    for i_batch, data in enumerate(train_dataloader, 1):\n",
    "        mfcc1, mfcc2, label = data['spec1'], data['spec2'], data['label']\n",
    "        mfcc1 = Variable(mfcc1.float(), requires_grad=True).to(device)\n",
    "        mfcc2 = Variable(mfcc2.float(), requires_grad=True).to(device)\n",
    "        label = Variable(label.float(), requires_grad=True).to(device)\n",
    "                \n",
    "        output1, output2 = model(mfcc1.float(), mfcc2.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output1, output2, label.float())\n",
    "        \n",
    "#         assert mfcc1.dim() == mfcc2.dim() == 4        \n",
    "#         assert output1.dim() == output2.dim() == 2\n",
    "#         assert loss.requires_grad and output1.requires_grad and output2.requires_grad\n",
    "#         assert loss.grad_fn is not None and output1.grad_fn is not None and output2.grad_fn is not None \n",
    "        \n",
    "#         print(\"loss\", loss, loss.requires_grad, loss.grad_fn)\n",
    "#         print(\"output1\", output1.shape, output1.requires_grad, output1.grad_fn, output1.device)\n",
    "#         print(\"output2\", output2.shape, output2.requires_grad, output2.grad_fn, output2.device)\n",
    "\n",
    "        loss.backward()\n",
    "            \n",
    "#         assert mfcc1.requires_grad and mfcc2.requires_grad                \n",
    "#         for name, param in model.named_parameters():\n",
    "#             assert param.requires_grad and param.grad is not None, (name, param.requires_grad, param.grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        if i_batch % int(n_batches/20) == 0:\n",
    "            print(\"Epoch {}/{}  Batch {}/{} \\nCurrent Batch Loss {}\\n\".format(epoch, N_EPOCHS, i_batch, n_batches, loss.item()))\n",
    "        \n",
    "    epoch_loss = running_loss / len(voxceleb_dataset)\n",
    "    print(\"==> Epoch {}/{} Epoch Loss {}\".format(epoch, N_EPOCHS, epoch_loss.item()))\n",
    "\n",
    "    is_best = epoch_loss < best_loss\n",
    "    if is_best:\n",
    "        best_loss = epoch_loss\n",
    "        \n",
    "        save_checkpoint({'epoch': epoch,\n",
    "                         'state_dict': model.state_dict(),\n",
    "                         'optim_dict': optimizer.state_dict()},\n",
    "                        loss=epoch_loss)\n",
    "    else:\n",
    "        print(\"### Epoch Loss did not improve\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "o_8nmcBcIPJw",
    "outputId": "f94a84f1-710b-4d93-af4b-0f887668f53a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1731a2dda0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAHMSoKDNKrd"
   },
   "outputs": [],
   "source": [
    "if is_best:\n",
    "    save_checkpoint({'epoch': epoch,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'optim_dict': optimizer.state_dict()}, \n",
    "                    is_best=is_best, loss=epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKzVCuTsQhO0"
   },
   "outputs": [],
   "source": [
    "load_saved_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Part_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
